= Développer avec les RDD

== Initialiser Spark
* L'interaction avec Spark débute par la création d'une instance de https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.html[`SparkContext`]
* Le contexte est créé à partir d'un objet https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkConf.html[`SparkConf`] qui configure l'application
+
[source,python]
----
from pyspark import SparkConf, SparkContext

conf = SparkConf().setMaster("local").setAppName("My app")
sc = SparkContext(conf=conf)
----

* Dans un environnement interactif (`pyspark` ou notebook), le contexte est en général créé automatiquement
** la variable représentant le contexte est nommée `sc`

== Qu'est-ce qu'un RDD ?
* Un _Resilient Distributed Datasets_ (RDD) est une collection partitionnée et immuable d'enregistrements
* Un RDD est
** en lecture seul
** tolérant aux pannes (_resilient_)
** distribué (partitions réparties sur les nœuds du cluster)
* La tolérance aux pannes est assurée en maintenant la _lignée_ (_lineage_) de chaque RDD
* Un RDD n'est pas nécessairement matérialisé (_évaluation paresseuse_)
* Il est possible de contrôler la https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence[_stratégie de stockage_] d'un RDD ainsi que le _critère de partitionnement_ (et le nombre de partitions)

== Création d'un RDD
* Un RDD peut être créé à partir de données externes ou d'un autre RDD
** à partir d'une collection avec `parallelize`
+
[source,python]
----
dist_data = sc.parallelize([1, 2, 3, 4, 5])
----
** à partir d'un fichier texte avec `textFile`
+
[source,python]
----
dist_file = sc.textFile("data.txt")
----
** à partir d'un fichier binaire (`sequenceFile`, `hadoopRDD`)
* Spark peut ouvrir directement un répertoire, un fichier compressé ou un ensemble de fichiers

== Opérations sur un RDD
* Un https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html[RDD] supporte deux types d'opérations : les _transformations_ et les _actions_
[horizontal]
_transformation_:: retourne un nouvel RDD par transformation du RDD courant
_action_:: déclenche le calcul d'une valeur sur un RDD
* Certaines opérations supposent un RDD particulier :
contenant des tuples (clé, valeur) (https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.fullOuterJoin.html#pyspark.RDD.fullOuterJoin[fullOuterJoin], …),
contenant des flottants (https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.mean.html#pyspark.RDD.mean[mean], …)
* Un RDD peut être rendu persistant (`persist` ou `cache`) pour être réutilisé ultérieurement

== Quelques transformations
[horizontal]
`filter`:: conserve les éléments validant le prédicat
`map`:: applique une fonction sur chaque élément
`flatMap`:: idem `map` mais désimbrique les collections
`distinct`:: supprime les doublons
`sample`:: extrait un échantillon aléatoire
`groupByKey`:: regroupe selon les valeurs de clé
`join`:: réalise la jointure entre deux RDDs

== Quelques actions
[horizontal]
`reduce`:: applique une fonction de réduction
`collect`:: retourne un tableau formé des éléments
`count`:: retourne le nombre d'éléments
`take`:: retourne un tableau des <n> premiers éléments
`takeSample`:: retourne un tableau de <n> éléments
`saveAsTextFile`:: sauve les éléments au format texte
`countByKey`:: compte le nombre d'occurences de chaque clé

== Exemples
.la somme des tailles des lignes d'un fichier
[source,python]
----
lines = sc.textFile("data.txt") #<1>
lineLengths = lines.map(lambda s: len(s))  #<2>
totalLength = lineLengths.reduce(lambda a, b: a + b)  #<3>
----
<1> https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.textFile.html#pyspark.SparkContext.textFile[`SparkContext.textFile`]: les lignes du fichier `data.txt` sont chargées dans un RDD (`→ pyspark.rdd.RDD[str]`)
<2> https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.map.html#pyspark.RDD.map[`RDD.map`]: la fonction anonyme est appliquée à chaque élément du RDD (`→ pyspark.rdd.RDD[int]`)
<3> https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.reduce.html#pyspark.RDD.reduce[`RDD.reduce`]: la fonction anonyme est utilisée pour réduire les éléments du RDD (`→ int`)

.nombre d'occurences de chaque ligne
[source,python]
----
from operator import add
lines = sc.textFile("data.txt")
pairs = lines.map(lambda s: (s, 1)) #<1>
counts = pairs.reduceByKey(add) #<2>
result = sorted(counts.collect()) #<3>
----
<1> (`→ pyspark.rdd.RDD[tuple(str, int)]`)
<2> https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.reduceByKey.html#pyspark.RDD.reduceByKey[`RDD.reduceByKey`]: applique la fonction anonyme aux valeurs associées à chaque clé (`→ pyspark.rdd.RDD[tuple(str, int)]`)
<3> https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.collect.html#pyspark.RDD.collect[`RDD.collect`]: transforme le RDD en liste Python (`→ pyspark.rdd.RDD[list(tuple(str, int))]`)

== Shuffle
* Certaines opérations déclenchent un https://spark.apache.org/docs/latest/rdd-programming-guide.html#shuffle-operations[_shuffle_] (mélange)
* Un _shuffle_ redistribue les données dans les partitions
* Cette opération est couteuse en terme d'I/O

== Persistance des RDD
* Un RDD peut être rendu persistant pour être réutilisé (`persist` ou `cache`)
* Par défaut, un RDD est persistant en mémoire
* La stratégie de persistance est définie en passant un objet https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.storage.StorageLevel[`StorageLevel`] à `persist` (`cache` rend uniquement persistant en mémoire)
** `MEMORY_ONLY`, `MEMORY_AND_DISK`, `DISK_ONLY`, ...

== Références
* https://amplab.cs.berkeley.edu/publication/resilient-distributed-datasets-a-fault-tolerant-abstraction-for-in-memory-cluster-computing/[Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing], Matei et al., NSDI'2012.
* https://spark.apache.org/docs/latest/rdd-programming-guide.html[Spark Programming Guide]

== Exercices
* Ouvrir, exécuter et compléter les cellules des notebooks `notebooks/00_prise_en_main.ipynb`, `notebooks/05_preparation_des_donnees.ipynb` et `notebooks/10_introduction_aux_rdd.ipynb`
